import pandas as pd
import os
import re
from datetime import datetime
import random 
import shutil
import numpy as np 
import json 

# --- è¨­å®šé …ç›®ï¼ˆã“ã“ã ã‘ã€ãã¾ã¡ã‚ƒã‚“ã®ç’°å¢ƒã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ã­ï¼ï¼‰ ---
# æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€
APP_ROOT_DIR = r'C:\Users\User26\yoko\dev\csvRead_arProcessor' # å£²æ›é‡‘ã‚¢ãƒ—ãƒªã®ãƒ«ãƒ¼ãƒˆ

# B*030.csvã‚’ã‚³ãƒ”ãƒ¼ã—ãŸå…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã®ãƒ‘ã‚¹
INPUT_BASE_DIR = r'G:\å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–\å•†å·¥ä¸­é‡‘\202412_å‹˜å®šç§‘ç›®æ˜ç´°æœ¬ç•ªç¨¼åƒ\50_æ¤œè¨¼\010_åå¯¾å‹˜å®šæ€§èƒ½è©•ä¾¡\20_ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿\ä½œæˆãƒ¯ãƒ¼ã‚¯\20_å£²æ›é‡‘\Import' 

# åŠ å·¥å¾Œã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ (æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹)
PROCESSED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'processed_output') 
# ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ (æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹)
MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data')
# ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ (æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹)
MERGED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'merged_output') 


# â˜…â˜…â˜… FINAL_POSTGRE_COLUMNS ã‚’ãŠå®¢æ§˜ãŒæç¤ºã—ãŸå£²æ›é‡‘ç”¨ã®ãƒªã‚¹ãƒˆã«å®Œå…¨ã«ä¸€è‡´ã•ã›ã‚‹ï¼ã“ã‚ŒãŒçœŸã®æœ€çµ‚å½¢ï¼ â˜…â˜…â˜…
FINAL_POSTGRE_COLUMNS = [
    'ocr_result_id', 
    'page_no', 
    'id', 
    'jgroupid_string', 
    'cif_number', 
    'settlement_at',
    'registration_number',          # æ–°è¦
    'calculation_name_original',    # æ–°è¦
    'calculation_name',             # æ–°è¦
    'partner_name_original',        # æ–°è¦
    'partner_name',                 # æ–°è¦
    'partner_location_original',    # æ–°è¦
    'partner_location',             # æ–°è¦
    'partner_location_prefecture',  # æ–°è¦
    'partner_location_city',        # æ–°è¦
    'partner_location_town',        # æ–°è¦
    'partner_location_block',       # æ–°è¦
    'partner_com_code',             # æ–°è¦ (maker_com_codeã«è©²å½“)
    'partner_com_code_status_id',   # æ–°è¦
    'partner_comcd_relation_source_type_id', # æ–°è¦
    'partner_exist_comcd_relation_history_id', # æ–°è¦
    'balance_original',             
    'balance',
    'description_original',         
    'description'                   
]
# æ³¨æ„: maker_name_original, maker_name, maker_com_code, issue_date_original, issue_date, due_date_original, due_date
#       balance_original, balance, paying_bank_name_original, paying_bank_name, paying_bank_branch_name_original,
#       paying_bank_branch_name, discount_bank_name_original, discount_bank_name ã¯ã€
#       å—å–æ‰‹å½¢ã‚¢ãƒ—ãƒªã® FINAL_POSTGRE_COLUMNS ã«å­˜åœ¨ã—ãŸãŒã€å£²æ›é‡‘ã‚¢ãƒ—ãƒªã®æ–°ã—ã„ãƒªã‚¹ãƒˆã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ã€‚
#       æ–°ã—ã„ãƒªã‚¹ãƒˆã‚’å®Œå…¨ã«æ¡ç”¨ã™ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã¯å‡¦ç†å¯¾è±¡å¤–ã¨ãªã‚‹ã€‚
#       maker_com_code ã¯ partner_com_code ã«åå‰ãŒå¤‰ã‚ã£ãŸã¨è§£é‡ˆã—ã€ãƒªã‚¹ãƒˆã‚’æ›´æ–°ã€‚
#       ã‚‚ã—maker_com_codeã¨partner_com_codeãŒåˆ¥ã®æ„å‘³ãªã‚‰ã€ãƒªã‚¹ãƒˆã‚’å†åº¦èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
#       ä»Šå›ã¯ã€Œpartner_com_codeãŒå—å–æ‰‹å½¢ã§ã®maker_com_codeã«è©²å½“ã€ã¨ã„ã†æŒ‡ç¤ºã‚’å„ªå…ˆã€‚


# --- å„CSVãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã”ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾© ---
# â˜…â˜…â˜… å£²æ›é‡‘ (B*030.csv) ã®ãŸã‚ã®å°‚ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ â˜…â˜…â˜…
# å…ƒãƒ‡ãƒ¼ã‚¿: ç§‘ç›®,ç›¸æ‰‹å…ˆåç§°(æ°å),ç›¸æ‰‹å…ˆæ‰€åœ¨åœ°(ä½æ‰€),æœŸæœ«ç¾åœ¨é«˜,æ‘˜è¦
ACCOUNTS_RECEIVABLE_MAPPING_DICT = {
    # Postgreã‚«ãƒ©ãƒ å : å…ƒCSVãƒ˜ãƒƒãƒ€ãƒ¼å
    'calculation_name': 'ç§‘ç›®',           # calculation_name ã¯ 'ç§‘ç›®' ã‹ã‚‰
    'partner_name': 'ç›¸æ‰‹å…ˆåç§°(æ°å)',    # partner_name ã¯ 'ç›¸æ‰‹å…ˆåç§°(æ°å)' ã‹ã‚‰
    'partner_location': 'ç›¸æ‰‹å…ˆæ‰€åœ¨åœ°(ä½æ‰€)', # partner_location ã¯ 'ç›¸æ‰‹å…ˆæ‰€åœ¨åœ°(ä½æ‰€)' ã‹ã‚‰
    'balance': 'æœŸæœ«ç¾åœ¨é«˜',              # balance ã¯ 'æœŸæœ«ç¾åœ¨é«˜' ã‹ã‚‰
    'description': 'æ‘˜è¦',               # description ã¯ 'æ‘˜è¦' ã‹ã‚‰
    # registration_number, partner_location_prefecture, city, town, block ãªã©ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥å¯¾å¿œã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚ã€
    # å¾Œç¶šã®æ´¾ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ã§ãƒ–ãƒ©ãƒ³ã‚¯ã«ãªã‚‹ã‹ã€åˆ¥ã®æ–¹æ³•ã§ç”Ÿæˆã•ã‚Œã‚‹ã€‚
}

# ä»¥å‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚‚å®šç¾©ã¯æ®‹ã™ (ä»Šå›ã¯ ACCOUNTS_RECEIVABLE_MAPPING_DICT ã‚’ä½¿ã†)
HAND_BILL_MAPPING_DICT = {} # å£²æ›é‡‘ã‚¢ãƒ—ãƒªã§ã¯ä½¿ç”¨ã—ãªã„ãŸã‚ç©ºã«ã™ã‚‹ã‹ã€å‰Šé™¤ã—ã¦ã‚‚è‰¯ã„
FINANCIAL_STATEMENT_MAPPING_DICT = {}
LOAN_DETAILS_MAPPING_DICT = {}
NO_HEADER_MAPPING_DICT = {}


# --- é–¢æ•°å®šç¾© ---
ocr_id_mapping = {}
_ocr_id_sequence_counter = 0 
_ocr_id_fixed_timestamp_str = "" 

def get_ocr_result_id_for_group(file_group_root_name): 
    global ocr_id_mapping
    global _ocr_id_sequence_counter
    global _ocr_id_fixed_timestamp_str

    if file_group_root_name not in ocr_id_mapping:
        sequence_part_int = _ocr_id_sequence_counter * 10
        if sequence_part_int > 99999: 
            sequence_part_int = sequence_part_int % 100000 
        sequence_part_str = str(sequence_part_int).zfill(5) 
        new_ocr_id = f"{_ocr_id_fixed_timestamp_str}{sequence_part_str}" 

        ocr_id_mapping[file_group_root_name] = new_ocr_id
        _ocr_id_sequence_counter += 1
    
    return ocr_id_mapping[file_group_root_name]

maker_name_to_com_code_map = {} # partner_com_code ç”¨ã«æµç”¨
next_maker_com_code_val = 100 

def get_partner_com_code_for_name(partner_name): # maker_com_code ã®ä»£ã‚ã‚Š
    """
    partner_nameã«åŸºã¥ã„ã¦3æ¡ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¡ç•ªãƒ»å–å¾—ã—ã€å…ˆé ­ã« '3' ã‚’ä»˜ã‘ã¦4æ¡ã«ã™ã‚‹ã€‚
    åŒã˜partner_nameã«ã¯åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚
    """
    global maker_name_to_com_code_map # è¾æ›¸åã‚’æµç”¨
    global next_maker_com_code_val 

    partner_name_str = str(partner_name).strip() 
    
    if not partner_name_str: 
        return "" 

    if partner_name_str in maker_name_to_com_code_map:
        return maker_name_to_com_code_map[partner_name_str]
    else:
        new_code_int = next_maker_com_code_val % 1000 
        if new_code_int < 100: 
            new_code_int = 100 + new_code_int 
        
        # â˜…â˜…â˜… å¤‰æ›´ç‚¹: ç”Ÿæˆã•ã‚ŒãŸ3æ¡ã‚³ãƒ¼ãƒ‰ã®å…ˆé ­ã« '3' ã‚’è¿½åŠ ã—ã¦4æ¡ã«ã™ã‚‹ â˜…â˜…â˜…
        new_code_4digit = '3' + str(new_code_int).zfill(3) 
        
        maker_name_to_com_code_map[partner_name_str] = new_code_4digit 
        next_maker_com_code_val += 1
        return new_code_4digit

# jgroupid_string ã¯ '001' å›ºå®šãªã®ã§é–¢æ•°ã¯ä¸è¦

def process_universal_csv(input_filepath, processed_output_base_dir, input_base_dir, 
                        maker_master_df, ocr_id_map_for_groups, current_file_group_root_name, 
                        final_postgre_columns_list, no_header_map, hand_bill_map, financial_map, loan_map,
                        accounts_receivable_map): # æ–°ã—ã„ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’å¼•æ•°ã«è¿½åŠ 
    """
    å…¨ã¦ã®AIReadå‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€çµ±ä¸€ã•ã‚ŒãŸPostgreSQLå‘ã‘ã‚«ãƒ©ãƒ å½¢å¼ã«å¤‰æ›ã—ã¦å‡ºåŠ›ã™ã‚‹é–¢æ•°ã€‚
    CSVã®ç¨®é¡ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å†…å®¹ï¼‰ã‚’åˆ¤åˆ¥ã—ã€ãã‚Œãã‚Œã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã€‚
    """
    df_original = None
    file_type = "ä¸æ˜" 
    
    try:
        encodings_to_try = ['utf-8', 'utf-8-sig', 'shift_jis', 'cp932']
        
        for enc in encodings_to_try:
            try:
                # header=0 ã§èª­ã¿è¾¼ã¿ã€dtype=str ã§å…¨ã¦æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
                df_original = pd.read_csv(input_filepath, encoding=enc, header=0, sep=',', quotechar='"', 
                                        dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                
                df_original.columns = df_original.columns.str.strip() 
                current_headers = df_original.columns.tolist()

                # â˜…â˜…â˜… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®åˆ¤åˆ¥ãƒ­ã‚¸ãƒƒã‚¯ã‚’ B*030.csv (å£²æ›é‡‘æƒ…å ±) ã«æœ€é©åŒ– â˜…â˜…â˜…
                is_accounts_receivable = ('ç§‘ç›®' in current_headers) and ('ç›¸æ‰‹å…ˆåç§°(æ°å)' in current_headers) and ('æœŸæœ«ç¾åœ¨é«˜' in current_headers) and ('æ‘˜è¦' in current_headers)
                
                if is_accounts_receivable: 
                    file_type = "å£²æ›é‡‘æƒ…å ±"
                # ãã®ä»– (æ‰‹å½¢æƒ…å ±ãªã©) ã®åˆ¤å®šã¯ã“ã“ã§ã¯è¡Œã‚ãªã„ã€ã¾ãŸã¯ç°¡ç•¥åŒ–
                # ã“ã®ã‚¢ãƒ—ãƒªã¯B*030.csvå°‚ç”¨ãªã®ã§ã€ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¯æƒ³å®šã—ãªã„
                else:
                    file_type = "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—" # å£²æ›é‡‘å½¢å¼ã«åˆã‚ãªã„å ´åˆã¯æ±ç”¨ã¨ã—ã¦å‡¦ç†ã‚’è©¦ã¿ã‚‹
                    df_original = pd.read_csv(input_filepath, encoding=enc, header=None, sep=',', quotechar='"', 
                                            dtype=str, na_values=['ã€ƒ'], keep_default_na=False)
                    df_original.columns = df_original.columns.astype(str).str.strip() 
                
                print(f"  ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã®åˆ¤å®šçµæœ: '{file_type}'")
                print(f"  ãƒ‡ãƒãƒƒã‚°: èª­ã¿è¾¼ã‚“ã  df_original ã®ã‚«ãƒ©ãƒ :\n{df_original.columns.tolist()}")
                print(f"  ãƒ‡ãƒãƒƒã‚°: èª­ã¿è¾¼ã‚“ã  df_original ã®æœ€åˆã®3è¡Œ:\n{df_original.head(3).to_string()}") 
                print(f"  ãƒ‡ãƒãƒƒã‚°: df_originalå†…ã®æ¬ æå€¤ (NaN) ã®æ•°:\n{df_original.isnull().sum().to_string()}") 
                    
                break 
            except Exception as e_inner: 
                print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ {enc} ã§èª­ã¿è¾¼ã¿å¤±æ•—ã€‚åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e_inner}")
                df_original = None 
                continue 

        if df_original is None or df_original.empty:
            print(f"  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã‚’ã©ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šã§ã‚‚èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return 
        
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã¯ '{file_type}' ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆ{input_filepath}ï¼‰: CSVèª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¤åˆ¥ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return

    # --- ãƒ‡ãƒ¼ã‚¿åŠ å·¥å‡¦ç† ---
    df_data_rows = df_original.copy() 

    if df_data_rows.empty:
        print(f"  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(input_filepath)} ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€åŠ å·¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 

    # ã€Œã€ƒã€ãƒãƒ¼ã‚¯ã®ã¿ã‚’ffillã§åŸ‹ã‚ã€ç©ºæ–‡å­—åˆ—ã¯ãã®ã¾ã¾ç¶­æŒ
    df_data_rows = df_data_rows.ffill() 
    df_data_rows = df_data_rows.fillna('') 
    print(f"  â„¹ï¸ ã€Œã€ƒã€ãƒãƒ¼ã‚¯ã‚’ç›´ä¸Šãƒ‡ãƒ¼ã‚¿ã§åŸ‹ã‚ã€å…ƒã€…ãƒ–ãƒ©ãƒ³ã‚¯ã ã£ãŸç®‡æ‰€ã¯ç¶­æŒã—ã¾ã—ãŸã€‚")

    # åˆè¨ˆè¡Œã®å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯
    keywords_to_delete = ["åˆè¨ˆ", "å°è¨ˆ", "è¨ˆ", "æ‰‹æŒæ‰‹å½¢è¨ˆ", "å‰²å¼•æ‰‹å½¢è¨ˆ", "ãã®ä»–è¨ˆ"] # "ãã®ä»–è¨ˆ" ã‚’è¿½åŠ 
    
    filter_conditions = []
    keywords_regex = r'|'.join([re.escape(k) for k in keywords_to_delete]) 
    
    # â˜…â˜…â˜… å£²æ›é‡‘å½¢å¼ã®å ´åˆã®åˆè¨ˆè¡Œå‰Šé™¤ã®å¯¾è±¡ã‚«ãƒ©ãƒ  â˜…â˜…â˜…
    if file_type == "å£²æ›é‡‘æƒ…å ±":
        if 'ç§‘ç›®' in df_data_rows.columns: # ç§‘ç›®ã‚’ãƒã‚§ãƒƒã‚¯
            filter_conditions.append(df_data_rows['ç§‘ç›®'].str.contains(keywords_regex, regex=True, na=False))
        elif 'ç›¸æ‰‹å…ˆåç§°(æ°å)' in df_data_rows.columns: # å¿µã®ãŸã‚ç›¸æ‰‹å…ˆåç§°ã‚‚ãƒã‚§ãƒƒã‚¯
            filter_conditions.append(df_data_rows['ç›¸æ‰‹å…ˆåç§°(æ°å)'].str.contains(keywords_regex, regex=True, na=False))
    elif file_type == "æ±ç”¨ãƒ‡ãƒ¼ã‚¿_ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—":
        if '0' in df_data_rows.columns: 
            filter_conditions.append(df_data_rows['0'].str.contains(keywords_regex, regex=True, na=False))
    # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆæ‰‹å½¢ã€è²¡å‹™è«¸è¡¨ã€å€Ÿå…¥é‡‘ï¼‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ä»Šå›ã¯ç„¡è¦–ã—ã€å£²æ›é‡‘å°‚ç”¨ã«ã™ã‚‹

    if filter_conditions:
        combined_filter = pd.concat(filter_conditions, axis=1).any(axis=1)
        rows_deleted_count = combined_filter.sum()
        df_data_rows = df_data_rows[~combined_filter].reset_index(drop=True)
        if rows_deleted_count > 0:
            print(f"  â„¹ï¸ åˆè¨ˆè¡Œï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³: {keywords_regex}ï¼‰ã‚’ {rows_deleted_count} è¡Œå‰Šé™¤ã—ã¾ã—ãŸã€‚")
    
    num_rows_to_process = len(df_data_rows) 
    
    # df_processed ã®åˆæœŸåŒ–
    df_processed = pd.DataFrame('', index=range(num_rows_to_process), columns=final_postgre_columns_list)


    # --- å…±é€šé …ç›® (PostgreSQLã®ã‚°ãƒªãƒ¼ãƒ³ã®è¡¨ã®å·¦å´ã€è‡ªå‹•ç”Ÿæˆé …ç›®) ã‚’ç”Ÿæˆ ---
    df_processed['ocr_result_id'] = [get_ocr_result_id_for_group(current_file_group_root_name)] * num_rows_to_process 

    df_processed['page_no'] = [1] * num_rows_to_process 

    df_processed['id'] = range(1, num_rows_to_process + 1)

    df_processed['jgroupid_string'] = ['001'] * num_rows_to_process

    cif_number_val = current_file_group_root_name[1:] 
    df_processed['cif_number'] = [cif_number_val] * num_rows_to_process

    settlement_at_val = datetime.now().strftime('%Y%m') 
    df_processed['settlement_at'] = [settlement_at_val] * num_rows_to_process


    # --- å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ ---
    mapping_to_use = {}
    if file_type == "å£²æ›é‡‘æƒ…å ±": # å£²æ›é‡‘å½¢å¼ã®å ´åˆã€å°‚ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨
        mapping_to_use = ACCOUNTS_RECEIVABLE_MAPPING_DICT
    else: # å£²æ›é‡‘å½¢å¼ä»¥å¤–ã¯æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ï¼ˆã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã»ã¼ç™ºç”Ÿã—ãªã„æƒ³å®šï¼‰
        mapping_to_use = NO_HEADER_MAPPING_DICT # ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—æ±ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°

    df_data_rows.columns = df_data_rows.columns.astype(str) # å¿µã®ãŸã‚strã«å¤‰æ›
    
    for pg_col_name, src_ref in mapping_to_use.items():
        source_data_series = None
        if isinstance(src_ref, str): 
            if src_ref in df_data_rows.columns: 
                source_data_series = df_data_rows[src_ref]
            else:
                print(f"  âš ï¸ è­¦å‘Š: ãƒãƒƒãƒ”ãƒ³ã‚°å…ƒã®ã‚«ãƒ©ãƒ  '{src_ref}' ãŒå…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆPostgreSQLã‚«ãƒ©ãƒ : {pg_col_name}ï¼‰ã€‚ã“ã®ã‚«ãƒ©ãƒ ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã«ãªã‚Šã¾ã™ã€‚")
        elif isinstance(src_ref, int): 
            if str(src_ref) in df_data_rows.columns: 
                source_data_series = df_data_rows[str(src_ref)]
            elif src_ref < df_data_rows.shape[1]: 
                source_data_series = df_data_rows.iloc[:, src_ref]
            else:
                print(f"  âš ï¸ è­¦å‘Š: ãƒãƒƒãƒ”ãƒ³ã‚°å…ƒã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{src_ref}' ãŒå…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆPostgreSQLã‚«ãƒ©ãƒ : {pg_col_name}ï¼‰ã€‚ã“ã®ã‚«ãƒ©ãƒ ã¯ãƒ–ãƒ©ãƒ³ã‚¯ã«ãªã‚Šã¾ã™ã€‚")

        if source_data_series is not None:
            df_processed[pg_col_name] = source_data_series.astype(str).values 
        else:
            pass 


    # --- Excelé–¢æ•°ç›¸å½“ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨ï¼ˆæ´¾ç”Ÿã‚«ãƒ©ãƒ ã®ç”Ÿæˆï¼‰ ---
    # â˜…â˜…â˜… ãŠå®¢æ§˜ãŒæç¤ºã—ãŸ23ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆã¨ã€å…ƒãƒ‡ãƒ¼ã‚¿ä¾‹ã«åŸºã¥ã„ã¦æ´¾ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ã‚’å†æ§‹ç¯‰ â˜…â˜…â˜…
    
    # registration_number ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ãªã„ãŒã€Postgreã‚«ãƒ©ãƒ ã«ã‚ã‚‹ã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç©ºã®ã¾ã¾
    df_processed['registration_number'] = df_processed['registration_number'].fillna('').astype(str)

    # calculation_name_original, calculation_name
    # calculation_name ã¯ ACCOUNTS_RECEIVABLE_MAPPING_DICT ã§ 'ç§‘ç›®' ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿
    df_processed['calculation_name_original'] = df_processed['calculation_name'].copy()

    # partner_name_original, partner_name
    # partner_name ã¯ ACCOUNTS_RECEIVABLE_MAPPING_DICT ã§ 'ç›¸æ‰‹å…ˆåç§°(æ°å)' ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿
    df_processed['partner_name_original'] = df_processed['partner_name'].copy()

    # partner_location_original, partner_location
    # partner_location ã¯ ACCOUNTS_RECEIVABLE_MAPPING_DICT ã§ 'ç›¸æ‰‹å…ˆæ‰€åœ¨åœ°(ä½æ‰€)' ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿
    df_processed['partner_location_original'] = df_processed['partner_location'].copy()
    
    # partner_location_prefecture, partner_location_city, partner_location_town, partner_location_block
    # ã“ã‚Œã‚‰ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥å¯¾å¿œã™ã‚‹ã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚ã€ãƒ–ãƒ©ãƒ³ã‚¯ã®ã¾ã¾ã¨ãªã‚‹

    # partner_com_code ã®å‡¦ç† (maker_com_code ã«è©²å½“)
    df_processed['partner_com_code'] = df_processed['partner_name'].apply(get_partner_com_code_for_name)
    # partner_com_code_status_id, partner_comcd_relation_source_type_id, partner_exist_comcd_relation_history_id
    # ã“ã‚Œã‚‰ã® ID ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥å¯¾å¿œã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç©ºã®ã¾ã¾ã¨ãªã‚‹

    # issue_date_original, issue_date, due_date_original, due_date
    # ã“ã‚Œã‚‰ã®ã‚«ãƒ©ãƒ ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥å¯¾å¿œã™ã‚‹ã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç©ºã®ã¾ã¾ã¨ãªã‚‹

    # balance_original, balance
    # balance ã¯ ACCOUNTS_RECEIVABLE_MAPPING_DICT ã§ 'æœŸæœ«ç¾åœ¨é«˜' ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿
    def clean_balance_no_comma(value): # å—å–æ‰‹å½¢ã‚¢ãƒ—ãƒªã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†åˆ©ç”¨
        try:
            cleaned_value = str(value).replace(',', '').strip()
            if not cleaned_value:
                return '' 
            numeric_value = float(cleaned_value)
            return str(int(numeric_value)) 
        except ValueError:
            return '' 
    
    df_processed['balance'] = df_processed['balance'].apply(clean_balance_no_comma)
    df_processed['balance_original'] = df_processed['balance'].copy() 

    # discount_bank_name_original, discount_bank_name
    # ã“ã‚Œã‚‰ã®ã‚«ãƒ©ãƒ ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥å¯¾å¿œã™ã‚‹ã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç©ºã®ã¾ã¾ã¨ãªã‚‹

    # description_original, description
    # description ã¯ ACCOUNTS_RECEIVABLE_MAPPING_DICT ã§ 'æ‘˜è¦' ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿
    df_processed['description_original'] = df_processed['description'].copy() 
    
    # â˜…â˜…â˜… ä¿®æ­£ã“ã“ã¾ã§ â˜…â˜…â˜…
    
    # --- ä¿å­˜å‡¦ç† ---
    # processed_output_sub_dir ã¯ APP_ROOT_DIR/processed_output/ ã¨ãªã‚‹
    # INPUT_BASE_DIR ã«ã¯ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„ã®ã§ã€ç›´æ¥ processed_output_base_dir ã«ä¿å­˜
    processed_output_filename = os.path.basename(input_filepath).replace('.csv', '_processed.csv')
    processed_output_filepath = os.path.join(processed_output_base_dir, processed_output_filename) # ç›´æ¥ base_dir ã«ä¿å­˜
    
    os.makedirs(processed_output_base_dir, exist_ok=True) # ãƒ•ã‚©ãƒ«ãƒ€ãŒç¢ºå®Ÿã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    df_processed.to_csv(processed_output_filepath, index=False, encoding='utf-8-sig')

    print(f"âœ… åŠ å·¥å®Œäº†: {input_filepath} -> {processed_output_filepath}")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    print(f"--- å‡¦ç†é–‹å§‹: {datetime.now()} ({APP_ROOT_DIR}) ---") # ã‚¢ãƒ—ãƒªåã‚’ãƒ­ã‚°ã«è¿½åŠ 
    
    # ocr_result_id ã®æ™‚åˆ»éƒ¨åˆ†ã‚’ã“ã“ã§ä¸€åº¦ã ã‘ç”Ÿæˆã—ã€å›ºå®šã™ã‚‹ï¼
    _ocr_id_fixed_timestamp_str = datetime.now().strftime('%Y%m%d%H%M')
    print(f"  â„¹ï¸ OCR IDç”Ÿæˆã®å›ºå®šæ™‚åˆ»: {_ocr_id_fixed_timestamp_str}")

    os.makedirs(PROCESSED_OUTPUT_BASE_DIR, exist_ok=True) 
    # MERGED_OUTPUT_BASE_DIR ã¯ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ä½œæˆã—ãªã„ (ãƒãƒ¼ã‚¸ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒä½œæˆã™ã‚‹)

    MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data') 
    os.makedirs(MASTER_DATA_DIR, exist_ok=True) # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã‚‚ä½œæˆ

    # master.csv ã¨ jgroupid_master.csv ã¯ã€ä»Šå›ã¯å£²æ›é‡‘ã‚¢ãƒ—ãƒªå°‚ç”¨ãªã®ã§ã€
    # ã‚‚ã—å—å–æ‰‹å½¢ã‚¢ãƒ—ãƒªã¨å…±é€šã§ä½¿ã†ãªã‚‰ã€ã“ã“ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹ã‹ã€
    # å£²æ›é‡‘ã‚¢ãƒ—ãƒªå°‚ç”¨ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã€‚
    # ä»Šå›ã¯ã€master_dataãƒ•ã‚©ãƒ«ãƒ€ã«å­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚
    maker_master_filepath = os.path.join(MASTER_DATA_DIR, 'master.csv') 
    maker_master_df = pd.DataFrame() 
    if os.path.exists(maker_master_filepath):
        try:
            maker_master_df = pd.read_csv(maker_master_filepath, encoding='utf-8')
            print(f"  â„¹ï¸ {maker_master_filepath} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯maker_com_codeç”Ÿæˆã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“)ã€‚")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {maker_master_filepath} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            maker_master_df = pd.DataFrame() 
    else:
        print(f"âš ï¸ è­¦å‘Š: {maker_master_filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (maker_com_codeç”Ÿæˆã«ã¯å½±éŸ¿ã‚ã‚Šã¾ã›ã‚“)ã€‚")
        maker_master_df = pd.DataFrame() 


    jgroupid_master_filepath = os.path.join(MASTER_DATA_DIR, 'jgroupid_master.csv')
    jgroupid_values_from_master = [] 
    if os.path.exists(jgroupid_master_filepath): 
        try:
            df_jgroupid_temp = pd.read_csv(jgroupid_master_filepath, encoding='utf-8', header=None)
            if not df_jgroupid_temp.empty and df_jgroupid_temp.shape[1] > 0:
                jgroupid_values_from_master = df_jgroupid_temp.iloc[:, 0].astype(str).tolist()
                if not jgroupid_values_from_master:
                    raise ValueError("jgroupid_master.csv ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã¾ã—ãŸãŒã€ãƒªã‚¹ãƒˆãŒç©ºã§ã™ã€‚")
            else:
                raise ValueError("jgroupid_master.csv ãŒç©ºã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: jgroupid_master.csv ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            jgroupid_values_from_master = [] # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ç©ºãƒªã‚¹ãƒˆã§ç¶™ç¶š
    else:
        print(f"âš ï¸ è­¦å‘Š: {jgroupid_master_filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        jgroupid_values_from_master = [] # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ

    # INPUT_BASE_DIR ãŒ copy_filtered_csv_arProcessor.py ã®ã‚³ãƒ”ãƒ¼å…ˆã¨ãªã‚‹ãƒ•ã‚©ãƒ«ãƒ€
    # ã“ã“ã‹ã‚‰ process_universal_csv ãŒãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    # B*030.csv ã®ã‚³ãƒ”ãƒ¼å…ˆãƒ•ã‚©ãƒ«ãƒ€
    INPUT_CSV_FILES_DIR = INPUT_BASE_DIR # INPUT_BASE_DIR ã‚’ç›´æ¥ä½¿ã†

    # ocr_result_id ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’äº‹å‰ã«ç”Ÿæˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåæŠ½å‡ºï¼‰
    print("\n--- ocr_result_id ãƒãƒƒãƒ”ãƒ³ã‚°äº‹å‰ç”Ÿæˆé–‹å§‹ ---")
    ocr_id_mapping = {}
    _ocr_id_sequence_counter = 0 
    
    all_target_file_groups_root = set() 
    for root, dirs, files in os.walk(INPUT_CSV_FILES_DIR): # INPUT_CSV_FILES_DIR ã‚’ã‚¦ã‚©ãƒ¼ã‚¯
        for filename in files:
            if filename.lower().endswith('.csv') and not filename.lower().endswith('_processed.csv'):
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã€ã‚’æŠ½å‡º (ä¾‹: B000001)
                # B000001_2.jpg_030.csv -> B000001
                match = re.match(r'^(B\d{6})_.*\.jpg_030\.csv$', filename, re.IGNORECASE) # B*030.csv ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆã‚ã›ã‚‹
                if match:
                    all_target_file_groups_root.add(match.group(1)) 
                else:
                    print(f"  â„¹ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆè‡´ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ã¯ocr_result_idç”Ÿæˆå¯¾è±¡å¤–ã§ã™ã€‚")
                    
    sorted_file_groups_root = sorted(list(all_target_file_groups_root)) 
    
    for group_root_name in sorted_file_groups_root:
        get_ocr_result_id_for_group(group_root_name) 
    
    print("--- ocr_result_id ãƒãƒƒãƒ”ãƒ³ã‚°äº‹å‰ç”Ÿæˆå®Œäº† ---")
    print(f"ç”Ÿæˆã•ã‚ŒãŸ ocr_result_id ãƒãƒƒãƒ”ãƒ³ã‚° (æœ€åˆã®5ã¤): {list(ocr_id_mapping.items())[:5]}...")

    # ç”Ÿæˆã—ãŸ ocr_id_mapping ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    ocr_id_map_filepath = os.path.join(MASTER_DATA_DIR, 'ocr_id_mapping_arProcessor.json') # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚¢ãƒ—ãƒªã”ã¨ã«å¤‰æ›´
    try:
        with open(ocr_id_map_filepath, 'w', encoding='utf-8') as f:
            json.dump(ocr_id_mapping, f, ensure_ascii=False, indent=4)
        print(f"  âœ… ocr_id_mapping ã‚’ {ocr_id_map_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ocr_id_mapping ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ¡ã‚¤ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ«ãƒ¼ãƒ—
    for root, dirs, files in os.walk(INPUT_CSV_FILES_DIR): # INPUT_CSV_FILES_DIR ã‚’ã‚¦ã‚©ãƒ¼ã‚¯
        for filename in files:
            if filename.lower().endswith('.csv') and not filename.lower().endswith('_processed.csv'): 
                input_filepath = os.path.join(root, filename)
                print(f"\n--- å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {input_filepath} ---")

                # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã‚’æŠ½å‡º
                current_file_group_root_name = None
                match = re.match(r'^(B\d{6})_.*\.jpg_030\.csv$', filename, re.IGNORECASE) # B*030.csv ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
                if match:
                    current_file_group_root_name = match.group(1) 
                
                if current_file_group_root_name is None:
                    print(f"  âš ï¸ è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue 

                process_universal_csv(input_filepath, PROCESSED_OUTPUT_BASE_DIR, INPUT_CSV_FILES_DIR, # INPUT_BASE_DIR ã‚’ INPUT_CSV_FILES_DIR ã«å¤‰æ›´
                                    maker_master_df, ocr_id_mapping, current_file_group_root_name, 
                                    FINAL_POSTGRE_COLUMNS, NO_HEADER_MAPPING_DICT, HAND_BILL_MAPPING_DICT, 
                                    FINANCIAL_STATEMENT_MAPPING_DICT, LOAN_DETAILS_MAPPING_DICT,
                                    ACCOUNTS_RECEIVABLE_MAPPING_DICT) # ACCOUNTS_RECEIVABLE_MAPPING_DICT ã‚’æ¸¡ã™

    print(f"\nğŸ‰ å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®åŠ å·¥å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ({datetime.now()}) ğŸ‰")
    