import pandas as pd
import os
import re
import shutil 
from datetime import datetime 
import json # ocr_id_mapping ã®èª­ã¿è¾¼ã¿ã«å¿…è¦

# è¨­å®šé …ç›®
APP_ROOT_DIR = r'C:\Users\User26\yoko\dev\csvRead_arProcessor' # å£²æ›é‡‘ã‚¢ãƒ—ãƒªã®ãƒ«ãƒ¼ãƒˆ

# åŠ å·¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ (process_data_arProcessor.py ã®å‡ºåŠ›å…ˆ)
PROCESSED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'processed_output') 
# ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
MERGED_OUTPUT_BASE_DIR = os.path.join(APP_ROOT_DIR, 'merged_output') 
# ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆocr_id_mapping.json ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹å ´æ‰€ï¼‰
MASTER_DATA_DIR = os.path.join(APP_ROOT_DIR, 'master_data')

# PostgreSQLã®æœ€çµ‚ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆï¼ˆprocess_data_arProcessor.py ã¨å®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰
FINAL_POSTGRE_COLUMNS = [
    'ocr_result_id', 
    'page_no', 
    'id', 
    'jgroupid_string', 
    'cif_number', 
    'settlement_at',
    'registration_number_original', 
    'registration_number',          
    'calculation_name_original',    
    'calculation_name',             
    'partner_name_original',        
    'partner_name',                 
    'partner_location_original',    
    'partner_location',             
    'partner_location_prefecture',  
    'partner_location_city',        
    'partner_location_town',        
    'partner_location_block',       
    'partner_com_code',             
    'partner_com_code_status_id',   
    'partner_comcd_relation_source_type_id', 
    'partner_exist_comcd_relation_history_id', 
    'balance_original',             
    'balance',
    'description_original',         
    'description',                  
    'conf_registration_number', 
    'conf_calculation_name',    
    'conf_partner_name',        
    'conf_partner_location',    
    'conf_balance',             
    'conf_description',         
    'coord_x_registration_number', 
    'coord_y_registration_number', 
    'coord_h_registration_number', 
    'coord_w_registration_number', 
    'coord_x_calculation_name', 
    'coord_y_calculation_name', 
    'coord_h_calculation_name', 
    'coord_w_calculation_name', 
    'coord_x_partner_name',     
    'coord_y_partner_name',     
    'coord_h_partner_name',     
    'coord_w_partner_name',     
    'coord_x_partner_location', 
    'coord_y_partner_location', 
    'coord_h_partner_location', 
    'coord_w_partner_location', 
    'coord_x_balance',          
    'coord_y_balance',          
    'coord_h_balance',          
    'coord_w_balance',          
    'coord_x_description',      
    'coord_y_description',      
    'coord_h_description',      
    'coord_w_description',
    'row_no',   
    'insertdatetime',           
    'updatedatetime',           
    'updateuser'                
]


def merge_processed_csv_files():
    """
    processed_output ãƒ•ã‚©ãƒ«ãƒ€å†…ã®åŠ å·¥æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«çµåˆã—ã€
    merged_output ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚
    """
    print(f"--- ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®çµåˆå‡¦ç†é–‹å§‹ (AR Processor) ---")
    print(f"åŠ å·¥æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚©ãƒ«ãƒ€: {PROCESSED_OUTPUT_BASE_DIR}")
    print(f"çµåˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {MERGED_OUTPUT_BASE_DIR}")

    # çµåˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    os.makedirs(MERGED_OUTPUT_BASE_DIR, exist_ok=True)

    files_to_merge_by_group = {}
    
    # processed_output ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚’å†å¸°çš„ã«æ¤œç´¢
    for root, dirs, files in os.walk(PROCESSED_OUTPUT_BASE_DIR): 
        for filename in files:
            # '_processed.csv' ã§çµ‚ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
            if filename.lower().endswith('_processed.csv'):
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã€ã¨ã€Œãƒšãƒ¼ã‚¸ç•ªå·ã€ã‚’æŠ½å‡º
                # ä¾‹: B000001_1.jpg_030_processed.csv -> group_root="B000001", page_num="1"
                match = re.match(r'^(B\d{6})_(\d+)\.jpg_030_processed\.csv$', filename, re.IGNORECASE) # B*030.csvç”¨
                if match:
                    group_root_name = match.group(1) # ä¾‹: B000001
                    page_num = int(match.group(2))   # ä¾‹: 1 (ãƒšãƒ¼ã‚¸ç•ªå·)
                    filepath = os.path.join(root, filename)

                    if group_root_name not in files_to_merge_by_group:
                        files_to_merge_by_group[group_root_name] = []
                    files_to_merge_by_group[group_root_name].append((page_num, filepath))
                else:
                    print(f"  â„¹ï¸ ãƒãƒ¼ã‚¸å¯¾è±¡å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ (ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸ä¸€è‡´): {filename}")

    merged_files_count = 0
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ«ãƒ¼ãƒˆåã§ã‚½ãƒ¼ãƒˆã—ã¦ã€çµåˆé †ã‚’ä¿è¨¼
    sorted_merged_groups = sorted(files_to_merge_by_group.keys())

    # ocr_id_mapping.json ã‚’èª­ã¿è¾¼ã‚€
    ocr_id_map_filepath = os.path.join(MASTER_DATA_DIR, 'ocr_id_mapping_arProcessor.json') # å£²æ›é‡‘ã‚¢ãƒ—ãƒªç”¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«å
    ocr_id_mapping_from_file = {}
    try:
        if os.path.exists(ocr_id_map_filepath):
            with open(ocr_id_map_filepath, 'r', encoding='utf-8') as f:
                ocr_id_mapping_from_file = json.load(f)
            print(f"  âœ… ocr_id_mapping_arProcessor.json ã‚’ {ocr_id_map_filepath} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        else:
            print(f"  âš ï¸ è­¦å‘Š: ocr_id_mapping_arProcessor.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚IDã®æœŸå¾…å€¤ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ocr_id_mapping_arProcessor.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    for group_root_name in sorted_merged_groups: # ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—åã§ãƒ«ãƒ¼ãƒ—
        page_files = files_to_merge_by_group[group_root_name]

        if not page_files: 
            continue 
        
        # ã“ã®ã‚°ãƒ«ãƒ¼ãƒ—ã® ocr_result_id, cif_number, jgroupid_string ã®ã€ŒæœŸå¾…å€¤ã€ã‚’è¨­å®š
        # ocr_id ã¯ ocr_id_mapping_from_file ã‹ã‚‰å–å¾—
        expected_ocr_id_for_group = ocr_id_mapping_from_file.get(group_root_name) 
        expected_cif_number_for_group = group_root_name[1:] # Bã‚’é™¤ã„ãŸ6æ¡
        expected_jgroupid_string_for_group = '001' # Jgroupidã¯å¸¸ã«001å›ºå®š
        
        combined_df = pd.DataFrame(columns=FINAL_POSTGRE_COLUMNS) 
        
        print(f"  â†’ ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆä¸­ (æœŸå¾…OCR ID: {expected_ocr_id_for_group})...")
        
        global_id_counter = 1 

        for page_index, (page_num, filepath) in enumerate(page_files):
            try:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€éš›ã«ã€IDã‚«ãƒ©ãƒ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å¼·åˆ¶æŒ‡å®š
                read_csv_dtype = {
                    'ocr_result_id': str,
                    'jgroupid_string': str,
                    'cif_number': str,
                    'partner_com_code': str # å£²æ›é‡‘ã‚¢ãƒ—ãƒªã§ã¯ partner_com_code ãŒã“ã‚Œã«è©²å½“
                }
                df_page = pd.read_csv(filepath, encoding='utf-8-sig', dtype=read_csv_dtype, na_values=['ã€ƒ'], keep_default_na=False)
                
                if df_page.empty: 
                    print(f"    â„¹ï¸ {os.path.basename(filepath)} ã¯ç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # IDæƒ…å ±ã®å¼·åˆ¶ä¸Šæ›¸ãï¼ˆprocess_data.py ã®çµæœã‚’ä¿¡é ¼ã—ã€ã“ã“ã§æœ€çµ‚çš„ã«å¼·åˆ¶ã™ã‚‹ï¼‰
                df_page['ocr_result_id'] = expected_ocr_id_for_group
                df_page['cif_number'] = expected_cif_number_for_group
                df_page['jgroupid_string'] = expected_jgroupid_string_for_group

                # çµåˆå‰ã«ã‚«ãƒ©ãƒ é †ã‚’FINAL_POSTGRE_COLUMNSã«åˆã‚ã›ã‚‹ï¼ˆé‡è¦ï¼‰
                df_page = df_page[FINAL_POSTGRE_COLUMNS] 

                df_page['id'] = range(global_id_counter, global_id_counter + len(df_page))
                global_id_counter += len(df_page) 

                df_page['page_no'] = 1 # page_no ã¯å…¨ã¦1å›ºå®š
                
                combined_df = pd.concat([combined_df, df_page], ignore_index=True)
                print(f"    - ãƒšãƒ¼ã‚¸ {page_num} ({os.path.basename(filepath)}) ã‚’çµåˆã—ã¾ã—ãŸã€‚")
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ {page_num} ã®ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(filepath)} ã®èª­ã¿è¾¼ã¿/çµåˆä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback 
                traceback.print_exc() 
                combined_df = pd.concat([combined_df, pd.DataFrame(columns=FINAL_POSTGRE_COLUMNS)], ignore_index=True)


        # çµåˆã•ã‚ŒãŸDataFrameã‚’æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
        merged_output_filename = f"{group_root_name}_merged.csv" 
        merged_output_filepath = os.path.join(MERGED_OUTPUT_BASE_DIR, merged_output_filename)
        
        # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ (å—å–æ‰‹å½¢ã‚¢ãƒ—ãƒªã® _processed_merged.csv ã¯å‰Šé™¤ã—ãªã„)
        # ã“ã®ã‚¢ãƒ—ãƒªã¯ _arProcessor ã®åå‰ã§é‹ç”¨ã•ã‚Œã‚‹ãŸã‚ã€éå»ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã¯è€ƒæ…®ã—ãªã„
        # ã‚‚ã—ã€B*030.csv ã§ã‚‚ _processed_merged.csv ãŒéå»ã«ä½œã‚‰ã‚Œã¦ã„ãŸãªã‚‰ã€
        # ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ãã®å ´åˆã€ä¸‹è¨˜ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ã€‚
        # old_filename_pattern = f"{group_root_name}_processed_merged.csv" 
        # old_filepath = os.path.join(MERGED_OUTPUT_BASE_DIR, old_filename_pattern)
        # if os.path.exists(old_filepath):
        #     try:
        #         os.remove(old_filepath)
        #         print(f"  âœ… å¤ã„ãƒ•ã‚¡ã‚¤ãƒ« '{old_filename_pattern}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        #     except Exception as e:
        #         print(f"  âŒ ã‚¨ãƒ©ãƒ¼: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ« '{old_filename_pattern}' ã®å‰Šé™¤ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")

        try:
            if not combined_df.empty: 
                combined_df.to_csv(merged_output_filepath, index=False, encoding='utf-8-sig', header=False) 
                merged_files_count += 1
                print(f"  âœ… ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®çµåˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {merged_output_filepath}")
            else:
                print(f"  âš ï¸ è­¦å‘Š: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã«çµåˆå¯¾è±¡ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ«ãƒ¼ãƒ— '{group_root_name}' ã®çµåˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\n--- ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®çµåˆå‡¦ç†å®Œäº† (AR Processor) ---")
    print(f"ğŸ‰ çµåˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {merged_files_count} ğŸ‰")

if __name__ == "__main__":
    print(f"--- çµåˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ (AR Processor): {datetime.now()} ---")
    merge_processed_csv_files()
    print(f"\nğŸ‰ å…¨ã¦ã®çµåˆå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ({datetime.now()}) ğŸ‰")
    